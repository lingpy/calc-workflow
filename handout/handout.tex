\documentclass[xetex,svgnames]{scrartcl}

% packages
\usepackage{xltxtra}
\usepackage{polyglossia}
\usepackage[
    urlcolor=black,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    pagecolor=black,
    linktocpage=true,
    ]{hyperref}
\usepackage{fontspec}
\usepackage{scrpage2}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}
\usepackage{colortbl,color,xcolor}
\usepackage{alltt}
\usepackage{listings}
\usepackage{multirow}

% fonts general
\setmainfont[Mapping=tex-text,Scale=1.0]{FreeSans}
\setsansfont[Mapping=tex-text,Scale=1.0]{FreeSans}
\setmonofont{FreeMono}

% special fonts
\newfontfamily\hana{HAN NOM A}
\newfontfamily\hanb{HAN NOM B}
\newfontfamily\sil{Charis SIL}





\newcommand{\White}[1]{\cellcolor{white} \textcolor{black}{ #1}}

% language settings
\setmainlanguage{english}
\setotherlanguage{german}

% pagestyle settings
\pagestyle{scrheadings}
\ihead{M.-S. Wu and J.-M. List}
\chead{Computer-Assisted Language Comparison}
\ohead{+++DATE+++}
\ifoot{}
\cfoot{\pagemark}
\ofoot{}

\usepackage[
    alldates=terse,
    backend=bibtex,
    %backref=true,
    bibstyle=authoryear,
    firstinits=true,
    ibidtracker=strict,
    isbn=false,doi=false,url=false,
    labelnumber=true,
    loccittracker=strict,
    hyperref=false,
    maxbibnames=10,
    maxcitenames=2,
    natbib=true,
    opcittracker=strict,
    sortcites=true,
    sorting=nyt,
    defernumbers=true,
    style=authoryear-ibid,
    terseinits=false
    ]{biblatex}

\input{bibstyle}
\newcommand{\Table}[1]{%
    \begin{flushleft}
        \begin{tabular}{|p{14.5cm}|}
            \hline \cellcolor{lightgray} \bf \pur #1
            \\\hline
        \end{tabular}
    \end{flushleft}
}

\bibliography{evobib.bib}

\usepackage{zhspacing}

\begin{document}
\zhspacing
%\maketitle

\begin{center}
    {\bfseries \huge  Computer-Assisted Language Comparison: State of the Art}
\end{center}

\begin{abstract}
  \small
  By comparing the languages of the world, we gain invaluable insights into human prehistory,
  predating the appearance of written records by thousands of years. The traditional methods for
  language comparison are based on manual data inspection. With more and more data available, they
  reach their practical limits. Computer applications, however, are not capable of replacing
  experts’ experience and intuition. In a situation where computers cannot replace experts and
  experts do not have enough time to analyse the massive amounts of data, a new framework, neither
  completely computer-driven, nor ignorant of the help computers provide, becomes urgent. Such
  frameworks are well-established in biology and translation, where computational tools cannot
  provide the accuracy needed to arrive at convincing results, but do assist humans to digest large
  data sets. In this talk, we will illustrate what we consider the current state of the art of
  computer-assisted language comparison, by presenting a workflow that starts from raw data and
  leads up to a stage where sound correspondence patterns across multiple languages have been
  identified and can be readily presented, inspected, and discussed. We illustrate this workflow
  with help of a dataset on Hmong-Mien languages, which has so far not yet been analyzed in this
  way. Our illustration is furthermore accompanied by Python code and instructions on how to make
  use of additional web-based tools we developed, so that users can replicate our workflow or apply
  it for their own purposes.

\end{abstract}

\section{Introduction}
\subsection{The Gap between Computational and Traditional Historical Linguistics}

The proposal of new, fancy, and shiny quantitative methods applied to handle problems in historical
linguistics has created a gap between what one could call ``classical" approaches to historical
language comparison and the ``new and innovative" automatic approaches.
Classical linguists are often skeptical of the new approaches, partly because the results differ from
those achieved by classical methods \citep{Anthony2015,Holm2007}, but also because the majority of the
new approaches work in a black box fashion and do not allow inspecting the concrete findings in
detail. Computational linguists, on the other hand, complain about classical historical linguists' lack
of consistency when applying the classical methods.

\subsection{Computer-Assisted Disciplines}

The use of computer applications in historical linguistics is steadily increasing. With more and
more data available, the classical methods reach their practical limits. At the same time, computer
applications are not capable of replacing experts' experience and intuition, especially when data
are sparse. If computers cannot replace experts and experts do not have enough time to analyse the
massive amounts of data, a new framework is needed, neither completely computer-driven, nor ignorant
of the assistance computers afford. Such computer-\emph{assisted} frameworks are well-established in
biology and translation. Current machine translation systems, for example, are efficient and
consistent, but they are by no means accurate, and no one would use them in place of a trained
expert. Trained experts, on the other hand, do not necessarily work consistently and efficiently. In
order to enhance both the quality of machine translation and the efficiency and consistency of human
translation, a new paradigm of computer-assisted translation has emerged \citep[3]{Barrachina2008}.

\subsection{Computer-Assisted Language Comparison}

Following the idea of computer-assisted frameworks in translation and biology, a framework for
computer-assisted language comparison (CALC) could be the key to reconcile classical and
computational approaches in historical linguistics. Computational approaches may still not be able
to compete with human experts, but when used to pre-process the data with human experts
systematically correcting the results, they can drastically increase both the efficiency and the
consistency of the classical comparative method.


\begin{figure}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{calc-figure-1.png}
  \caption{Basic idea of data managment within the CALC framework.}
  \label{fig:calc}
\end{figure}

The basic idea behind computer-\emph{assisted} as opposed to computer-\emph{based} language
comparison is to allow scholars to do qualitative and
quantitative research are done at the same time. In order to allow scholars to do this, \textbf{data
must always be available in \emph{machine-} and \emph{human-readable} form}.
Figure \ref{fig:calc} shows a tentative workflow for the CALC framework, in which data is constantly passed back and
forth between computational and classical linguists.

Three different aspects are essential for this workflow:
\begin{itemize}
  \item[(a)] New
software allows for the application of transparent methods which increase the accuracy and the application range
of current methods and also treat the peculiarities of specific language families (like, e.g.,
Sino-Tibetan).
\item[(b)] Interactive tools provide
an interface between human and machine, allowing experts to correct errors and to inspect the automatically
produced results in detail.
\item[(c)] Specific data is used to test and train the software algorithms.
  \end{itemize}



\section{Workflows for Computer-Assisted Language Comparison}

\subsection{Overview}
Our workflows for computer-assisted language comparison have so far been intensively tested on a
small set of 8 Burmish languages, which we investigated in collaboration with Nathan W. Hill, who
was responsible for the qualitative investigation of the data and for the common discussion of new
computer-assisted methods which were then implemented by Johann-Mattis List (see \citealt{Hill2017a}
for an exemplary discussion of some of the new approaches). Our experience with the
Burmish project by now allows us to set up a first workflow that starts from raw data and leads up
to the explicit identification of correspondence patterns across multiple languages. At the moment,
List and Hill develop the workflow further to account also for (semi)-automatic reconstructions, but
in this talk, only the identification of correspondence patterns will be discussed.

\subsection{Details of the Workflow}

Our workflow currently comprises 5 different stages, in which we successively lift linguistic data
from their raw form in which we can find them in wordlists and tables published in dictionaries and
field-work notes, up to a level where correspondence patterns across cognate words have been
automatically identified and can be qualitatively inspected by the scholar.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{calc-workflow.pdf}
  \caption{Current state-of-the-art workflow developed in collaboration of different research groups
  working in computer-assisted frameworks.}
  \label{fig:workflow}
\end{figure}

Although the workflow can be carried out almost completely without any manual intervention by a
linguist, we emphasize that this workflow explicitly \emph{allows} for expert intervention at
\emph{any} of the five stages. While, in our experience, specific care is required when lifting the
data the first time to machine-readable format, it should further be noted that \emph{all} steps of
the workflow profit from human intervention, since none of the automatic methods currently available
to us could spot all patterns in linguistic data without over- or underestimating their importance
for linguistic reconstruction.

Our workflow starts from \emph{raw data}, including tabular data from fieldwork notes or data
published in books and articles, which we re-organize and re-format in such a way that the data can
be processed by our tools. Once we have \emph{machine-readable data}, we can use methods for
automatic cognate detection \citep{List2016g} in order to infer \emph{partial cognates} across the
languages in our data. Having inferred cognates, we can now also align the data in the cognate sets.
While we could use phonetic alignment approaches discussed in the literature \citep{List2014d}, we
now use a new approach, based on phonotactic templates, which has the advantage of being much
faster and accurate when dealing with alignments for South-East-Asian languages. Once having
identified the alignments, we start to search automatically for cognates \emph{across} different
concepts. Since all automatic methods \emph{need} to start searching for cognates within the same
concept slot (otherwise, there would be too many false positives), our new method, which makes used
of a systematic comparison of readily aligned cognate sets, systematically searches for cognates
independent of their meaning. The improved, cross-semantic cognate sets, which are all readily
aligned, have the specific property of being \emph{strict}: no cognate set could compare two
morphemes from the same language which would differ in their pronunciation. \citep{List2018PBLOG7}
calls these cognate sets \emph{regular}, but in discussions with Nathan Hill, we decided that
\emph{regular} is probably not the best term, as they can well be wrong, so we call them
\emph{strict} now. Once strict cognates have been identified, we use the new algorithm for the
automatic inference of sound correspondence patterns across multiple languages by \citet{List2019a}
to infer the correspondence patterns in the data.

In Section \ref{sec:wf}, we will provide detailed examples how all steps of the workflow interact,
using a relatively recent collection of linguistic data on Hmong-Mien languages \citep{Chen2012} for
this purpose.
\subsection{Materials and Methods for the Workflow Illustration}

The data we use to illustrate our workflow in the next section was originally collected by
\citet{Chen2012}, and later added in digital form to the Wiktionary project. Chén's collection of
\emph{frequent terms} (\emph{chángyòng cíbiǎo 常用词表}, pp. 567-862) comprises 885 different
concepts translated into 25 varieties of Hmong-Mien. In Figure \ref{fig:data}, we contrast one
exemplary page from Chéns book with the data as it has been prepared by the Wiktionary users.
We can see that the data is essentially the same, but that the rows and columns of the tabular form
have been swapped.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{chen-illustration.pdf}
  \caption{Contrasting Chén's original data with the table in Wiktionary}
  \label{fig:data}
\end{figure}

All methods have either been implemented and published before, or are shared along with the slides
and the handout for this talk. Since this is work in progress, however, we warn users that both data
and code will be in flux for some time, but we will make sure that both data and code can always be
readily analyzed with our tools. All code, the data we use, and installation instructions can be
found at \url{https://github.com/lingpy/calc-workflow}. We ask those interested in testing our
methods to use our issue-tracker on GitHub in case they face difficulties of any kind.
<<<<<<< HEAD
In this talk, we present the workflow with a subset of 10 varieties of the Hmong-Mien languages in
Chén's sample, for which we selected a subset of 313 concepts. The concepts were selected by
checking the overlap with the current 504 concept list of the Burmish Etymological Database project
(headed by Nathan W. Hill, data online at \url{https://dighl.github.io/burmish}). 
The languages were selected for some general reasons, like lexical coverage, geographic
distribution, or basic diversity, but not with the specific ``eye" of a historical linguist who
would select languages to explore the history of a language family. We would be glad about any additional
recommendations, if scholars feel competent to give us advice in this context.
The
geographic locations are shown in the Figure \ref{fig:geo}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{Geographic.pdf}
  \caption{Language geographic locations}
  \label{fig:geo}
\end{figure}

\section{Illustration of the Workflow}\label{sec:wf}
\subsection{From Raw Data to Segmented Data}
When searching for sound correspondence patterns, we can safely assume that the data is a wordlist; a lexical dataset contains vocabularies which is translated into various languages. The existing wordlists have various presentations, such as, the orientations of data or the usage of seperators of synonyms (\ref{fig:data}). Due to idiosyncratic formats, linguistic datasets often lack interoperability and are therefore not reusable. Following the \textit{Fair} guiding principles of scientific data management from Wilkinson et al., ``Findable, Accessible, Interoperable, and Reusable''\citep{Wilkinson2016}, we convert our raw data into \textit{Lingpy Wordlist} format. 
The format has the following guidelines:

\begin{itemize}
\item A tab-separated input file. 
\item First row serves as a header and defines the content of the rest of the rows. 
\item One value per cell, therefore, synonyms are divided into different rows.
\item Four mandatory columns : unique identification numbers for each row, the language name (DOCULECT), the comparison concept (CONCEPT), the original transcription (International phonetic alphabet, FORM or VALUE). 
\item TOKENS-columns should supply the supply the transcriptions in space-segmented form.
\end{itemize}

Many existing tools make use of \textit{Lingpy wordlist} format, including the tools we demonstrate here in this talk.

\subsection{From Segmented Data to Cognate Sets}\label{sec:pcogs}

Once the data is segmented and provided in the long table format as it is required by the LingPy
software package, as described in our tutorial \citep{List2018f}, we can use LingPy's partial
cognate detection method to infer partial cognates in our linguistic data. Partial cognates are
hereby understood as cognate assessments \emph{per morpheme} in our data, as opposed to cognate
assessments \emph{per word}. 
While it has always been clear to scholars working in the field of South-East Asian linguistics that
cognacy should rather be assigned on the level of the morpheme than on the level of full words,
given that the high degree of compounding would easily complicate the identification of cognate
relations, automatic methods, and specifically phylogenetic reconstruction approaches usually still
assume a rather naive one-word-one-cognate relation \citep{List2016f}. 

In our framework, we explicitly address this problem by adopting a numerical annotation format in
which each morpheme instead of each word form is assigned to a specific cognate set
\citep{Hill2017a}. This framework is illustrated in Figure \ref{fig:pcogs}, where we contrast word
forms for ``yesterday" in five Burmish varieties, indicating their detailed ``cognate relations". 
In the first ``traditional" style of cognate coding, we would proceed in a \emph{strict} way, only
allowing those words which are completely cognate in all their morphemes to be judged as cognates.
In the second, \emph{loose} cognate annotation, we judge all words that are in a \emph{connected
component} in our shared morpheme network to be cognate, and in the last column, we show our
explicit coding of partial cognacy, in which each morpheme is assigned to one cognate set.

\begin{figure}[htb]
  \centering
  \tabular{cc}
  \includegraphics[width=0.45\textwidth]{pcogs-1.png} &
  \includegraphics[width=0.45\textwidth]{pcogs-2.png} \\\endtabular
  \caption{Partial cognacy in Burmish language varieties and different ways of coding (see Hill and
  List 2017 and further explanations in the main text).
  coding.}
  \label{fig:pcogs}
\end{figure}

The software package LingPy offers a straightforward algorithm to detect and annotate partial
cognates in datasets formatted as long tables. This algorithm by \citet{List2016g} uses techniques for automatic sequence
comparison to create a network of similar morphemes for each meaning slot in a given dataset. It
then filters those concepts in consecutive stages, with the goal of avoiding that two or more
morphemes in the same word for the same language are assigned to the same cluster. In the end, the
algorithm outputs the cognate judgments in the same format as indicated above in Figure
\ref{fig:pcogs}, namely, but assigning each morpheme to a given number, with the number representing
that cognate set.

Note that this algorithm works quite well, although it is, of course, not infallible. It reaches
between 88 and 90 percent on a test datasets consisting of Bai dialects, Chinese dialects, and
dialects of Tujia. With more challenging datasets, the scores will surely drop, but we can expect
that the automatic cognate detection is in any case \emph{helpful}, as is easier to correct cognates
than to assign them from scratch.

In addition to the cognate detection algorithm, the EDICTOR web-based tool for computer-assisted
language comparison \citep{List2017d}, freely available at \url{http://edictor.digling.org}, can be
used to quickly inspect and correct computer-generated cognate sets, by providing a very convenient
interface that allows users to quickly assign morphemes to cognate sets. The interface is
illustrated in Figure \ref{fig:edipart}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{partial-cognates.png}
  \caption{Partial cognate annotation within the EDICTOR tool for the word for ``chin" in 10
  selected Hmong-Mien varieties.}
  \label{fig:edipart}
\end{figure}

\begin{center}
  \tabular{|p{14cm}|}\hline
  SUMMARY \\\hline
  \begin{itemize}
    \item For a realistic annotation of cognate sets, the annotation of partial cognates, by which
      morphemes are assigned to cognate sets, is the only realistic choice.
    \item Partial cognates can be automatically identified with help of software, openly available
      as part of the LingPy software library (\url{lingpy.org}, \citealt{List2018i}) and the
      algorithm by \citet{List2016g}.
    \item Partial cognates can be annotated consistently with help of the EDICTOR tool
      \citep{List2017d}, online available at \url{http://edictor.digling.org}.
    \item Partial cognates in these frameworks are assigned to morphemes occurring in words with the
      same meaning, both for algorithmic and for practical reasons.
  \end{itemize}\\\hline
  \endtabular
\end{center}


\begin{itemize}
\item One morpheme correspondent to one cognate id.
\end{itemize}


\subsection{From Cognate Sets to Alignments}

\subsection{From Alignments to Cross-Semantic Cognates}
As mentioned above in Section \ref{sec:pcogs}, the partial cognates are only identified for words
with the same meaning. This is being done for algorithmic reasons (it would become quite complex to
compare all morphemes against each other algorithmically), and for practical reasons, since we
believe that it is always better to start from the obvious and save etymologies in historical
linguistics, rather than to start from complex ones. Given that semantic shift is a phenomenon for
which we dispose of little knowledge with respect to its patterns, we agree explicitly with scholars
like \citet{Dybo2008} in emphasizing that we should always expect to find clear-cut etymologies
within words of the same meaning, even if we know that more etymologies could be find when searching
\emph{cross-semantically}, i.e., among words which differ with respect to their meanings.
 
There are only a few approaches that try to identify cognates across different concepts, and one
could say that the task of \emph{cross-semantic cognate detection} is still one of the open problems
in computational historical linguistics. Approaches proposed so far include a rather complex
workflow by \citet{Wahle2016}, who uses \emph{hidden Markov models} for sequence comparison, and
proxies on colexifications, drawn from the database by \citet{Dellert2017}, to infer cognates across
different meaning slots. As this task is not completely evaluated, and only described in a short
paper, it is difficult to access its usefulness for our purposes. Another approach is presented by
\citet{Arnaud2017}, who apply Support Vector Machines trained on form and semantic similarities of
word pairs along with a flat clustering algorithm to partition words into cognate sets. 
While this approach is publicly available and seems to yield promising results, we are not sure to
which degree it would help us with our very specific goals of lifting an initially ``raw" dataset
to a level where we can assess sound correspondence patterns across multiple languages, especially
since the algorithms the authors use for cognate detection do \emph{not} take regular sound
correspondences into account, and they are also \emph{not} sensitive to partial cognates.
 
Thus, instead of these previously proposed solutions, we propose our own, rather simple approach to
search for cross-semantic partial cognate sets in our data. This approach is based on the
well-observed fact that the majority of morphemes in South-East Asian languages with a certain
preference for compounding and a high degree of word formation, is highly \emph{promiscuous}
\citep[8f]{List2016h}, given
that they recur within different words, surfacing in the form of \emph{partial colexifications}
\citep[62]{Hill2017a}. The term \emph{partial colexification} hereby serves as a cover term for
morphemes recurring across the lexicon of a language, with no specific distinction being made if
they are polysemous or homophonous. 
 
Our search for partial colexifications would not allow us directly to identify cross-semantic
cognates consistently, given that sound change may yield different morpheme mergers across different
languages. As a result, we cannot take the information from one language alone, but have to smartly
summarize all the information on recurring morphemes we can find in our data. The solution for this
problem is nevertheless straightforward, and it builds on the idea to not only compare single words, as originally proposed in \citet{Hill2017a}, but to compare complete \emph{alignments} instead. As our data is already aligned, and we have identified
cognates in a first run, potentially even refined by experts, we can compare whole cognate sets that
contain \emph{identical words in the same language}. 

If two alignments are completely identical with respect to the words they contain, 
there is no reason
to assign them to different cognate sets, and we can directly assign them to the same cognate class.
Even if they are simply homophonous, the assumption of regular sound change will allow us to treat
them similarly if we reconstruct the words back to the ancestral language.

The problematic cases are those cases, where we have \emph{incomplete data}. And this is usually
rather the rule than the exception.  We often will encounter cases where we have two alignments
which are only filled in parts with data from the different languages, and we will usually have
\emph{missing data} for one or more of the languages in our sample in a given alignment. Thus, when
comparing two alignments with each other, we need to make sure that we have at least one word in one
language in common. 

As an example, consider the data on ``son'' and ``daughter'' in five language varieties of our
illustration data. As can be seen immediately, two languages show striking \emph{partial
colexifications} for the two concepts, Chuanqiandian and East Qiandong. In both cases, one morpheme
recurs in the words for the two concepts. In the other cases, we find different words, but if we
compare the overall cognacy, we can also see that all five languages share one cognate morpheme for
``son'' (corresponding to the Proto-Hmong-Mien {\sil *tu̯ɛn} in Ratliff's reconstruction), 
and three varieties share one cognate morpheme for ``daughter'' (corresponding to {\sil
*mphje\textsuperscript{D}} in Ratliff's 2010 reconstruction), with the morpheme for ``son''
occurring also in the words for ``daughter'' in East Qiandong and Chuanqiandian, as mentioned
before.\nocite{Ratliff2010}


\begin{table}[htb]
  \centering
  \resizebox{\textwidth}{!}{%
  \tabular{lllll}\hline
  \bfseries Language &
  \bfseries Concept &
  \bfseries Form &
  \bfseries Cognacy &
  \bfseries Cross-Semantic \\\hline\hline
  East Baheng                     & SON      & \sil \colorbox{lightgray}{taŋ³⁵}         & 1     & 1 \\
  East Baheng                     & DAUGHTER & \sil pʰje⁵³                              & 2     & 2 \\\hline
  West Baheng                     & SON      & \sil ʔa³/⁰ + \colorbox{lightgray}{taŋ³⁵} & 3 1   & 3 1 \\
  West Baheng                     & DAUGHTER & \sil ta⁵⁵ + qa³/⁰ + tʰjei⁵³              & 4 5 6 & 4 5 6 \\\hline
  Chuanqiandian                   & SON      & \sil \colorbox{lightgray}{to⁴³}          & 1     & 1\\
  Chuanqiandian                   & DAUGHTER & \sil ⁿtsʰai³³                            & 7     & 7 \\\hline
  Chuanqiandian (Central Guizhou) & SON      & \sil tə²/⁰ + \colorbox{lightgray}{tə̃²⁴}  & 8 1   & 8 1\\
  Chuanqiandian (Central Guizhou) & DAUGHTER & \sil \colorbox{lightgray}{tə̃²⁴} + ⁿpʰe⁴² & 9 2   & 1 2\\\hline
  East Qiandong                   & SON      & \sil \colorbox{lightgray}{tei²⁴}         & 1     & 1 \\
  East Qiandong                   & DAUGHTER & \sil \colorbox{lightgray}{tei²⁴} + pʰa³⁵ & 9 2   & 1 2\\\hline
\endtabular}
  \caption{Terms for ``son'' and ``daughter'' across five Hmong-Mien varieties.}
  \label{tab:son}
\end{table}

Our workflow for automatically identifying these cases of cognacy is a new algorithm for
cross-semantic cognate detection, developed first for the work in the Burmish Etymological
Dictionary project lead by Nathan W. Hill. In this workflow, we start from all aligned cognate sets
in our data, and then systematically compare all alignments with each other. Whenever two
alignments are \emph{compatible}, i.e., they have (1)~at least one morpheme in one language
occurring in both aligned cognate sets, which is (2)~identical, and (3)~no shared morphemes in two
alignments which are \emph{not} identical, we treat them as belonging to one and the same cognate
set. We iterate over all alignments in the data algorithmically, merging the alignments into larger
sets in a greedy fashion, and re-assign cognate sets in the data. 

The results can be easily
inspected with help of the EDICTOR tool, for example, by inspecting cognate set distributions in
the data. When inspecting the cross-semantic cognates, which we label \texttt{CROSSIDS} in our data,
the tool will always show, which cognate sets span more than one concept, and users can directly
filter the data and look at the relevant instances. Among the 64 cognate sets reflected in all
languages in our sample, we find quite a few cross-semantically recurring morphemes, seven in total
(with many more for the whole data). The results are shown in Table \ref{tab:cross}.

\begin{table}[htb]
  \centering
  \tabular{llll}\hline
  Language &
  Concept &
  Form & 
  Morphemes \\\hline\hline
  East Baheng & NOSE & ⁿpjau³¹ & \texttt{NOSE} \\
  East Baheng & NASAL MUCUS & qa³/⁰ + ⁿpjau³¹ & \texttt{qa NOSE} \\\hline
  Western Luobuohe & TWO & ʔu³¹ & \texttt{TWO} \\\hline
  Western Luobuohe & TWENTY & ʔu³¹ + ʑo³¹ & \texttt{TWO zo} \\\hline
  Western Baheng & SON & ʔa³/⁰ + taŋ³⁵ & \texttt{SON} \\
  Western Baheng & SON-IN-LAW & taŋ³⁵ + wei³¹ & \texttt{SON wei} \\
  Western Baheng & GRANDSON & taŋ³⁵ + seŋ³¹ & \texttt{SON seng} \\\hline
  Eastern Qiandong & SUN &  qʰaŋ³³ + nei²⁴ & \texttt{po SUN} \\
  Eastern Qiandong & DAY (NOT NIGHT) & nei²⁴ & \texttt{SUN} \\\hline
  Western Baheng & FAECES (EXCREMENT) &  qa³¹ & \texttt{SHIT} \\
  Western Baheng & STOMACH & ʔa³/⁰ + tɕʰi³⁵ + qa³¹ & \texttt{a tci SHIT} \\\hline
  Western Qiandong & ANT & kæ̃⁴⁴ + mjɔ²² & \texttt{INSECT mjo} \\
  Western Qiandong & EARTHWORM &  kæ̃⁴⁴+tɕuŋ⁴⁴ & \texttt{INSECT tsung} \\\hline
  Eastern Baheng & BIRD & taŋ³⁵ + nuŋ³¹ & \texttt{BIRD-A BIRD-B} \\
  Eastern Baheng & NEST & ʑo¹¹ + taŋ³⁵ + nuŋ³¹ & \texttt{zo BIRD-A BIRD-B} \\\hline
  \endtabular
  \caption{Partial cognates among stable concepts with reflexes in all languages in our test
  datasets. We highlight shared cognates by giving a tentative gloss for them in capital letters in
  the column \emph{Morphemes}.}
  \label{tab:cross}
\end{table}

\subsection{From cross-semantic cognates to correspondence patterns}

\section{Discussion}

\section{Outlook}




\nocite{List2019a}

\printbibliography
\end{document}
